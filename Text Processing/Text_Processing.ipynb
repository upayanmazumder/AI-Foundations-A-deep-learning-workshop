{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6u981LRMV5U"
      },
      "source": [
        "# HTML Tag Removal\n",
        "\n",
        "In this notebook, we will explore different methods for removing HTML tags from text. HTML is often used for web content, and when extracting or processing text, it's common to remove the tags and keep only the plain text. We'll look at two main approaches:\n",
        "\n",
        "1. **Using Regular Expressions**  \n",
        "2. **Using the `BeautifulSoup` library**  \n",
        "\n",
        "After exploring these approaches, you'll find a small exercise to practice what you've learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNi4_8MiMYR4"
      },
      "source": [
        "## Why Remove HTML Tags?\n",
        "\n",
        "- When extracting data from web pages, we often obtain strings with HTML tags (e.g., `<p>`, `<div>`, etc.).\n",
        "- If we're performing text analytics (such as sentiment analysis, topic modeling, or keyword extraction), those tags become noise.\n",
        "- Removing HTML tags helps us get cleaner input data for NLP tasks or general text processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIJGVn-IG-92",
        "outputId": "1b7e7cd0-05f9-4a35-fb5f-845a56d1712f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original HTML Text:\n",
            "\n",
            "<html>\n",
            "  <head><title>Sample Page</title></head>\n",
            "  <body>\n",
            "    <p>This is a <strong>sample</strong> paragraph.</p>\n",
            "    <div class=\"content\">Another <a href='#'>link</a> here.</div>\n",
            "  </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Example text\n",
        "html_text = \"\"\"\n",
        "<html>\n",
        "  <head><title>Sample Page</title></head>\n",
        "  <body>\n",
        "    <p>This is a <strong>sample</strong> paragraph.</p>\n",
        "    <div class=\"content\">Another <a href='#'>link</a> here.</div>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original HTML Text:\")\n",
        "print(html_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI7SY7iGMdLi"
      },
      "source": [
        "## Approach 1: Using Regular Expressions\n",
        "\n",
        "A simple (though sometimes brittle) approach to removing HTML tags is to use a regular expression that matches anything in angle brackets (`< >`) and replaces it with an empty string. Keep in mind that HTML can get very complex, and regex might not always capture every edge case, but it can be sufficient for simple scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeliDLjhMbJ6",
        "outputId": "9c8b76a3-7ad0-4b7b-85ca-f5850acd5d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned text (Regex):\n",
            "\n",
            "\n",
            "  Sample Page\n",
            "  \n",
            "    This is a sample paragraph.\n",
            "    Another link here.\n",
            "  \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Removing tags using regular expressions\n",
        "\n",
        "def remove_html_tags_regex(text):\n",
        "    # This regex pattern matches anything that starts with '<' and ends with '>'\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "regex_cleaned_text = remove_html_tags_regex(html_text)\n",
        "print(\"Cleaned text (Regex):\")\n",
        "print(regex_cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0LY_FfaMgjU"
      },
      "source": [
        "## Approach 2: Using BeautifulSoup\n",
        "\n",
        "`BeautifulSoup` is a popular Python library for parsing HTML and XML documents. It allows us to easily extract the text content without dealing directly with raw string operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifUpTJwQMjOY",
        "outputId": "bb3736a1-ab14-40f8-8b57-c741f032300c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned text (BeautifulSoup):\n",
            "\n",
            "\n",
            "Sample Page\n",
            "\n",
            "This is a sample paragraph.\n",
            "Another link here.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Removing tags using BeautifulSoup\n",
        "\n",
        "def remove_html_tags_bs(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "bs_cleaned_text = remove_html_tags_bs(html_text)\n",
        "print(\"Cleaned text (BeautifulSoup):\")\n",
        "print(bs_cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ__3KS8Mlqr"
      },
      "source": [
        "## Comparison of Methods\n",
        "\n",
        "- **Regex-based approach**:\n",
        "  - Pros: Quick, minimal dependencies.  \n",
        "  - Cons: Can fail with nested or malformed HTML; not guaranteed to handle all real-world HTML complexities.\n",
        "\n",
        "- **BeautifulSoup approach**:\n",
        "  - Pros: Specifically designed for parsing HTML; robust for many HTML structures.  \n",
        "  - Cons: Requires installing and importing an external library.  \n",
        "\n",
        "Choose the method that best fits your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmQPiLBBMtv3"
      },
      "source": [
        "##Excercise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fEITY4gtMrtH"
      },
      "outputs": [],
      "source": [
        "# Exercise Starter Code\n",
        "\n",
        "exercise_html = \"\"\"\n",
        "<h1 style=\"color:red;\">Hello World!</h1>\n",
        "<p>This is an <em>HTML</em> example with a <a href='http://example.com'>link</a>.</p>\n",
        "<span>Some malformed <tag> text</span\n",
        "\"\"\"\n",
        "\n",
        "# TODO 1: Create your own HTML string or use the above.\n",
        "# TODO 2: Use the remove_html_tags_regex and remove_html_tags_bs functions on your string.\n",
        "# TODO 3: Print and compare the outputs. Are there any edge cases?\n",
        "\n",
        "# Example (uncomment below and replace exercise_html with your own):\n",
        "# print(\"Regex approach:\\n\", remove_html_tags_regex(exercise_html))\n",
        "# print(\"\\nBeautifulSoup approach:\\n\", remove_html_tags_bs(exercise_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzxB9ZnEOALF"
      },
      "source": [
        "# Emoji Removal or Replacement with the `emoji` Library\n",
        "\n",
        "In this notebook, we'll learn how to remove or replace emojis in text using the `emoji` library. This library provides convenient functions to identify and handle emojis in Unicode strings.\n",
        "\n",
        "We'll focus on:\n",
        "1. **Removing emojis** (i.e., deleting them from the text).\n",
        "2. **Replacing emojis** with a placeholder token (e.g., `<EMOJI>`).\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDKPsbEJN682",
        "outputId": "5a3cd08a-7a1d-48ce-81c3-d30271c77b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Original Text:\n",
            "Hello world! ðŸ˜Š I love Python ðŸâ¤ï¸.\n"
          ]
        }
      ],
      "source": [
        "# If you haven't installed the emoji library, uncomment and run the following line:\n",
        "!pip install emoji\n",
        "\n",
        "import emoji\n",
        "\n",
        "# Sample text containing emojis\n",
        "text_with_emojis = \"Hello world! ðŸ˜Š I love Python ðŸâ¤ï¸.\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text_with_emojis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHsQX1K8OGVM"
      },
      "source": [
        "## Removing Emojis\n",
        "\n",
        "We can use the function `replace_emoji` from the `emoji` library to remove emojis by replacing them with an empty string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98obxtX9OI7y",
        "outputId": "a6636028-d4af-4cbd-8752-ff8d2eafa8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text After Removing Emojis:\n",
            "Hello world!  I love Python .\n"
          ]
        }
      ],
      "source": [
        "def remove_emojis(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove all emojis from the provided text by replacing them with an empty string.\n",
        "    \"\"\"\n",
        "    return emoji.replace_emoji(text, \"\")\n",
        "\n",
        "removed_emojis_text = remove_emojis(text_with_emojis)\n",
        "print(\"Text After Removing Emojis:\")\n",
        "print(removed_emojis_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqDWSGhOKWV"
      },
      "source": [
        "## Replacing Emojis with a Placeholder\n",
        "\n",
        "Instead of deleting them, sometimes it's useful to keep track of where emojis appearâ€”especially for analysis or token replacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V997FqhZOhBu",
        "outputId": "c4f9fb26-3cf8-4f3d-aaf3-6de76952f0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text After replacing Emojis:\n",
            "Hello world! :smiling_face_with_smiling_eyes: I love Python :snake::red_heart:.\n"
          ]
        }
      ],
      "source": [
        "def replace_emoji(text: str):\n",
        "  return emoji.demojize(text)\n",
        "\n",
        "replaced_emojis_text = replace_emoji(text_with_emojis)\n",
        "print(\"Text After replacing Emojis:\")\n",
        "print(replaced_emojis_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NavE6mQXS7oS"
      },
      "source": [
        "# Basic Text Preprocessing & Language Translation\n",
        "\n",
        "In this notebook, we will cover:\n",
        "1. **Stop Word Removal** (using `nltk`).\n",
        "2. **Stemming** and **Lemmatization** (using `nltk`).\n",
        "3. **Removing Digits** from text (using regular expressions).\n",
        "4. **Lowercasing** text.\n",
        "\n",
        "Let's begin by importing the necessary libraries and loading some sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-iWwF60S8yO",
        "outputId": "012062c1-f26b-4300-8ae9-d90a416e952a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK data (if you haven't before)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "This is a sample TEXT!\n",
        "It contains numbers like 1234 and 56.\n",
        "We'll remove STOPWORDS, digits, and then try some stemming/lemmatization.\n",
        "Finally, let's translate this text into French!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4i7knOTm7w"
      },
      "source": [
        "## Stop Word Removal\n",
        "\n",
        "Stop words are commonly used words (e.g., \"the\", \"is\", \"in\") that often don't add significant meaning to a text.\n",
        "We'll use NLTK's built-in list of English stop words and remove them from our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76o5dDYvToze",
        "outputId": "eb3f1119-dc70-4ac7-98a4-6943672d9f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text after Stop Word Removal:\n",
            "\n",
            "sample text contains numbers like remove stopwords digits try finally let translate text french\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Get the English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Filter out stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    # Reconstruct the string\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "text_no_stopwords = remove_stopwords(sample_text)\n",
        "print(\"Text after Stop Word Removal:\\n\")\n",
        "print(text_no_stopwords)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2f4tQtUdCH"
      },
      "source": [
        "## Stemming / Lemmatization\n",
        "\n",
        "- **Stemming**: Reduces words to their word stem, which may not be a proper word (e.g., \"studies\" -> \"studi\").\n",
        "- **Lemmatization**: Reduces words to a valid base form (lemma), considering the context (e.g., \"studies\" -> \"study\").\n",
        "\n",
        "We will demonstrate both using NLTKâ€™s PorterStemmer and WordNetLemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7pLKlTmUXjV",
        "outputId": "1443da4a-c897-4aa7-e571-c718c86c8e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            " \n",
            "This is a sample TEXT! \n",
            "It contains numbers like 1234 and 56. \n",
            "We'll remove STOPWORDS, digits, and then try some stemming/lemmatization. \n",
            "Finally, let's translate this text into French!\n",
            "\n",
            "Stemmed Text:\n",
            " sampl text contain number like remov stopword digit tri final let translat text french\n",
            "\n",
            "Lemmatized Text:\n",
            " sample text contains number like remove stopwords digit try finally let translate text french\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "stemmed_text = stem_text(text_no_stopwords)\n",
        "lemmatized_text = lemmatize_text(text_no_stopwords)\n",
        "\n",
        "print(\"Sample Text:\\n\", sample_text)\n",
        "print(\"Stemmed Text:\\n\", stemmed_text)\n",
        "print(\"\\nLemmatized Text:\\n\", lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVjg2M3LVmq_"
      },
      "source": [
        "##Excercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8lOZd2LVg4k"
      },
      "outputs": [],
      "source": [
        "# Removing Digits and Lowercasing. Try Yourself !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M21MQ7ONYfS2"
      },
      "source": [
        "# POS Tagging with displaCy Visualization\n",
        "\n",
        "In this example, we use [spaCy](https://spacy.io/) to perform Part-of-Speech (POS) tagging on a sample sentence. We then render the syntactic dependency parse (which includes POS information) using **displaCy** directly within a Jupyter environment.\n",
        "\n",
        "**Steps**:\n",
        "1. **Import spaCy** and load the English model (`en_core_web_sm`).\n",
        "2. **Create a Doc object** by processing a text string with `nlp(...)`.\n",
        "3. **Visualize** the parse (dependencies and POS tags) using `displacy.render`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "qU6f4jCBVtYa",
        "outputId": "d0e30540-bb41-4db4-cdf8-8d896c205818"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c2e4627dcf24440787a4942ae29380f3-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2e4627dcf24440787a4942ae29380f3-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2e4627dcf24440787a4942ae29380f3-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2e4627dcf24440787a4942ae29380f3-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2e4627dcf24440787a4942ae29380f3-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2e4627dcf24440787a4942ae29380f3-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2e4627dcf24440787a4942ae29380f3-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "displacy.render(doc, style=\"dep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1dcXbmEamEM"
      },
      "source": [
        "# One-Hot Encoding of Text\n",
        "\n",
        "In this notebook, we demonstrate:\n",
        "1. Creating and preprocessing a small corpus.\n",
        "2. Building a vocabulary of unique words.\n",
        "3. Generating one-hot vectors for words in any given string.\n",
        "\n",
        "**Why One-Hot Encoding?**  \n",
        "One-hot encoding converts each word into a vector of zeros with a single '1' indicating the position of that word in the vocabulary. This is a simple way to represent text numerically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7MSVptRZaa7",
        "outputId": "d598bfa3-d67a-46eb-a0a7-4149f9043734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyjtUPvCpPjM",
        "outputId": "bb286f2c-c9f3-4fde-9cbc-66e2994edfed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ]
        }
      ],
      "source": [
        "#Build the vocabulary\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "    for word in doc.split():\n",
        "        if word not in vocab:\n",
        "            count = count +1\n",
        "            vocab[word] = count\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqLjXU8lpTd4"
      },
      "outputs": [],
      "source": [
        "#Get one hot representation for any string based on this vocabulary.\n",
        "#If the word exists in the vocabulary, its representation is returned.\n",
        "#If not, a list of zeroes is returned for that word.\n",
        "def get_onehot_vector(somestring):\n",
        "    onehot_encoded = []\n",
        "    for word in somestring.split():\n",
        "        temp = [0]*len(vocab)\n",
        "        if word in vocab:\n",
        "            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGSgLWgypW2x",
        "outputId": "fa25477a-172c-4c1c-9427-b28c6e0e1a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "man bites dog\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(processed_docs[1])\n",
        "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our corpus."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "n6u981LRMV5U",
        "VC2f4tQtUdCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
